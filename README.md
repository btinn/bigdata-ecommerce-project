# ğŸ§  BigData E-commerce Project
## ğŸ“ Giá»›i thiá»‡u
Dá»± Ã¡n BigData E-commerce lÃ  má»™t há»‡ thá»‘ng phÃ¢n tÃ­ch vÃ  gá»£i Ã½ sáº£n pháº©m dá»±a trÃªn dá»¯ liá»‡u lá»›n, Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng cÃ¡c cÃ´ng nghá»‡ trong há»‡ sinh thÃ¡i Big Data.
Má»¥c tiÃªu cá»§a dá»± Ã¡n lÃ  khai thÃ¡c hÃ nh vi ngÆ°á»i dÃ¹ng thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ Ä‘á»ƒ:
  -  PhÃ¢n tÃ­ch xu hÆ°á»›ng mua sáº¯m (theo danh má»¥c, thÆ°Æ¡ng hiá»‡u, loáº¡i sá»± kiá»‡n).
  -  Huáº¥n luyá»‡n mÃ´ hÃ¬nh ALS (Alternating Least Squares) Ä‘á»ƒ gá»£i Ã½ sáº£n pháº©m cÃ¡ nhÃ¢n hÃ³a cho tá»«ng ngÆ°á»i dÃ¹ng.
  -  Cung cáº¥p giao diá»‡n trá»±c quan báº±ng Streamlit Dashboard, giÃºp dá»… dÃ ng theo dÃµi, trÃ¬nh bÃ y vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£.


## ğŸ—ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng
| ThÃ nh pháº§n                           | CÃ´ng nghá»‡                     |
| ------------------------------------ | ----------------------------- |
| **Xá»­ lÃ½ dá»¯ liá»‡u & Machine Learning** | Apache Spark (PySpark, MLlib) |
| **LÆ°u trá»¯ & Quáº£n lÃ½ dá»¯ liá»‡u**        | Hadoop HDFS                   |
| **Dashboard trá»±c quan**              | Streamlit + Plotly            |
| **NgÃ´n ngá»¯ láº­p trÃ¬nh**               | Python 3                      |
| **Triá»ƒn khai & MÃ´i trÆ°á»ng**          | Docker, Docker Compose        |


## âš™ï¸ Quy trÃ¬nh hoáº¡t Ä‘á»™ng
### 1.Ingest & lÃ m sáº¡ch dá»¯ liá»‡u:
Dá»¯ liá»‡u e-commerce (tá»« file ecommerce.csv) Ä‘Æ°á»£c náº¡p lÃªn HDFS vÃ  xá»­ lÃ½ bá»Ÿi Spark.

### 2.Tá»•ng há»£p & phÃ¢n tÃ­ch (Aggregation):
Táº¡o cÃ¡c báº£ng thá»‘ng kÃª nhÆ°:
**    agg_by_cat â€” Tá»•ng há»£p theo danh má»¥c.
    agg_by_brand â€” Tá»•ng há»£p theo thÆ°Æ¡ng hiá»‡u.
    agg_by_type â€” Tá»•ng há»£p theo loáº¡i sá»± kiá»‡n.**

### 3.Huáº¥n luyá»‡n mÃ´ hÃ¬nh ALS:
Sá»­ dá»¥ng Spark MLlib Ä‘á»ƒ xÃ¢y dá»±ng há»‡ thá»‘ng gá»£i Ã½ sáº£n pháº©m cÃ¡ nhÃ¢n hÃ³a cho tá»«ng user_id.

### 4.Hiá»ƒn thá»‹ káº¿t quáº£:
Káº¿t quáº£ Spark job Ä‘Æ°á»£c lÆ°u táº¡i data/outputs/.
Streamlit Dashboard Ä‘á»c dá»¯ liá»‡u nÃ y Ä‘á»ƒ hiá»ƒn thá»‹ biá»ƒu Ä‘á»“, báº£ng vÃ  gá»£i Ã½.

## ğŸ§° Cáº¥u trÃºc chÃ­nh
bigdata-ecommerce-project/
â”œâ”€â”€ dashboard/           # á»¨ng dá»¥ng Streamlit
â”œâ”€â”€ data/                # Dá»¯ liá»‡u thÃ´ + Ä‘áº§u ra
â”œâ”€â”€ spark_jobs/          # MÃ£ xá»­ lÃ½ Spark (ingest, train ALS, utils)
â”œâ”€â”€ scripts/             # Script tiá»‡n Ã­ch (put_to_hdfs.sh, spark_submit.sh)
â”œâ”€â”€ Dockerfile.spark     # Cáº¥u hÃ¬nh image Spark Worker + numpy
â”œâ”€â”€ docker-compose.yml   # Khá»Ÿi cháº¡y cluster Hadoop + Spark
â”œâ”€â”€ requirements.txt     # ThÆ° viá»‡n Python cho dashboard
â””â”€â”€ README.md            # TÃ i liá»‡u hÆ°á»›ng dáº«n


## ğŸš€ HÆ¯á»šNG DáºªN CHáº Y Cá»¤M BIG DATA (HADOOP + SPARK + STREAMLIT)

### 1) Bring up the cluster
```bash
docker compose build spark-worker-1 spark-worker-2

docker compose up -d
```

### 2) Add sample CSV
Máº«u dá»¯ liá»‡u Ä‘Ã£ cÃ³ sáºµn táº¡i:
 `data/raw/ecommerce_sample.csv`.

 Náº¿u muá»‘n Ä‘áº©y lÃªn HDFS, cháº¡y:
```bash
bash scripts/put_to_hdfs.sh
```

### 3) Run Spark jobs
```bash
bash scripts/spark_submit.sh ingest   # ingest + clean + parquet + aggregates
bash scripts/spark_submit.sh als      # train ALS model (recommendation)
```

Káº¿t quáº£ Ä‘áº§u ra sáº½ xuáº¥t hiá»‡n táº¡i: `data/outputs/` vÃ  trong HDFS: `hdfs:///warehouse/ecommerce/events_parquet`.

### 4) Run the dashboard
CÃ i thÆ° viá»‡n cáº§n thiáº¿t:
```bash
pip install -r requirements.txt
```
Sau Ä‘Ã³ khá»Ÿi Ä‘á»™ng dashboard:
```bash
streamlit run dashboard/app.py
```

